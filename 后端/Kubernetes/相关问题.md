[TOC]

#### Q1:滚动更新和回滚更新？

> 其实滚动更新就是更新软件包时并不直接更新到该软件包的最新版本，而是更新到临近的新版本，然后一步一步更新到最新版本，就像滚雪球一样，越滚越新。滚动更新的设计是为了避免某些软件包由于最新版本不稳定而导致的软件包出现问题。

> 你可以监控滚动更新的过程，如果某次的滚动更新导致一些软件无法正常使用(也就是通常意义的“滚挂”)，那么你可以回滚更新

> 滚动更新通常出现在软件或者是系统中。滚动更新与传统更新的不同之处在于：滚动更新不但提供了更新服务，而且通常还提供了滚动进度查询，滚动历史记录，以及最重要的回滚等能力。通俗的说就是具有系统或是软件的主动降级的能力。

> K8s: 滚动更新是一次只更新一小部分副本，成功后，再更新更多的副本，最终完成所有副本的更新。滚动更新的最大的好处是零停机，整个更新过程始终有副本在运行，从而保证了业务的连续性。
>
> 回滚: Kubernetes 都会记录下当前的配置，保存为一个 revision（版次），这样就可以回滚到某个特定 revision。

#### Q2:Chroot、container技术、VM

**VM**

> 虚拟机Virtual Machine实际上就是运行在电脑上的一台电脑。虚拟环境下可以运行多台虚拟机。
>
> 虚拟机说白了就是通关软件的方法来虚拟出一台电脑，是一个完全隔离环境的完整的计算机系统，一切风险操作都发生在该虚拟机之中。
>
> VM和实际计算机没什么区别。
>
> 虚拟机实际上是
>
> 「A software defined computer that is running on top of  a secialized operating system called  a hypervisor. A hypervisor is the software that sits between the physical hardware and the VM. The hypervisor is making sure each VM gets the resource is it needs in an orderly manner.」--「一种软件化的电脑，运行在一个叫Hypervisor的特殊的服务器系统上，Hypervisor 是一个作用在VM与实体机之间的软件。Hypervisor 时刻确保每个VM以有序的方式获取所需的资源。」
>
> 「In the case of Google Cloud Platform , the VM are running in the cloud, the cloud is a network of powerful servers running hypervisors and data centers all over the world.」-- 「在谷歌云平台的情况下，VM在云上运行。云是强大的服务器网络，运行着全世界的hypervisor和数据。」
>
> ![](/Users/samstephen/Documents/Typora/image/区块链项目-VM.jpg)
>
> 

#### Q3:LXC和Container的差别

[简述Linux容器和lxc](https://www.jianshu.com/p/63fea471bc43)

>  LXC（Linux Container）将Linux进程沙盒化，使得进程之间相互隔离，并且能够控制各进程的资源分配。容器有效地将由单个操作系统管理的资源划分到孤立的组中，以更好地在孤立的组之间平衡有冲突的资源使用需求。在隔离的同时还提供共享机制，以实现容器与宿主机的资源共享。lxc 用容器的方式仿真了一个类似虚拟机的操作体验，并避免了虚拟机额外的系统负载。lxc利用cgroup和namespace在linux应用层创建了一个“虚拟机”（隔离的裸露文件系统），无法有效支持跨主机之间的容器迁移、管理复杂（lxd解决了这些问题）。lxc和docker不同地方在于lxc包含完整的操作系统，是一个系统容器。

> Docker的底层使用了LXC来实现的，但docker对lxc封装，提供了更好的操作性和移植性。Docker现在已经开发了他们自己的直接使用核心namespaces和cgroup的工具：libcontainer。 Docker容器将应用和其依赖环境全部打包到一个单一对象中，在不包含完整的操作系统的情况下就能运行普通应用，更加轻量级，可移植性更好。所以它成为了PaaS（比如Kubernates）平台的基石。

#### Q4:负载均衡IPVS

#### Q5: Paxos算法 / borgmaster奇数副本的原因

[Paxos算法原理和过程解析](https://blog.csdn.net/wolf_love666/article/details/92832811)

> 其解决的问题就是在分布式系统中如何就某个值（决议）达成一致。
>
> Paxos算法的前提假设是不存在拜占庭将军问题，即：信道是安全的（信道可靠），发出的信号不会被篡改，因为Paxos算法是基于消息传递的。此问题由Lamport提出，它也是 Paxos算法的提出者。

> 分布式系统中一般是通过多副本来保证可靠性，而多个副本之间会存在数据不一致的情况。所以必须有一个一致性算法来保证数据的一致。Paxos算法就是解决这种分布式场景中的一致性问题。

> Paxos算法中有三个角色，proposer、acceptor、learners。具体实现中一个进程可以充当多种角色。Proposer负责提出提案，Acceptor负责对提案作出裁决（accept与否），learner负责学习提案结果。

> 还有一个很重要的概念叫提案（Proposal）。最终要达成一致的value就在提案里。只要Proposer发的提案被Acceptor接受（半数以上的Acceptor同意才行），Proposer就认为该提案里的value被选定了。Acceptor告诉Learner哪个value被选定，Learner就认为那个value被选定。只要Acceptor接受了某个提案，Acceptor就任务该提案里的value被选定了。

> 为了避免单点故障，会有一个Acceptor集合，Proposer想Acceptor集合发送提案，Acceptor集合中的每个成员都有可能同意该提案且每个Acceptor只能批准一个提案，只有当一半以上的成员同意了一个提案，就认为该提案被选定了。

> 为了避免产生“平票”而无法决定新的Proposal，因此需要奇数副本。

##### Prepare阶段

1.Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求。Pareper（N）

2.如果一个Acceptor收到一个编号为N的Prepare请求，如果小于它已经响应过的请求，则拒绝，不回应或回复error。若N大于该Acceptor已经响应过的所有Prepare请求的编号（maxN），那么它就会将它已经接受过（已经经过第二阶段accept的提案）的编号最大的提案作为响应反馈给Proposer，同时该Acceptor承诺不再接受任何编号小于N的提案。

##### accept阶段

1.如果一个Proposer收到半数以上Acceptor对其发出的编号为N的Prepare请求的响应，那么它就会发送一个针对[N,V]提案的Accept请求给半数以上的Acceptor。

2.如果Acceptor收到一个针对编号为N的提案的Accept请求，只要该Acceptor没有对编号大于N的Prepare请求做出过响应，它就接受该提案。

##### learn阶段

完成了提案的确认之后，对所有learner进行告知。

1.Acceptor接受了一个提案，就将该提案发送给所有的learnor。Learner能快速获取被选定的proposal，但是通信的次数为M*N

2.发送给各自的主learner，在由主learner通知其他learner。通信次数减少，但是会出现单点问题，主learner会出现故障。

3.发送给一个learner集合，再由该集合中的learner通知其他的learner。集合中的learner个数越多，可靠性越好，但网络通信复杂度越高。

- [x] 若Paxos算法已经经历过一个完整的阶段，则每个Acceptor都已经有了一个acceptV，那么在阶段一，它们都会返回acceptV给Proposer，这样的话，新的V还怎么更新？

  > 个人想法：如果经历过一个完整的阶段还能继续下去，那说明此时有超过一半的Acceptor的值是不一样的，也就是没有选出超过一半相同的值。这时候一个新的大编号的proposer进行prepare阶段， 1.Acceptor会判断这个proposer的编号是比之前的小，则直接拒绝并返回失败（正常情况不可能，因为编号递增） 2.这个proposer编号比之前的大，则直接承诺（promise）会接受这个proposer的提议并且会拒绝比他编号小的proposer的请求，并且将自己已经有的acceptV以及此acceptV对应的提议编号返回给proposer。 3.（这是重点。）这时候可能会出现很多不同的值和编号，proposer会在下一阶段accept阶段选取其中编号最大的对应的那个值发送给acceptor。（这样基本能保证你后面写入的能继承前面写入的。“后者认同前者”的原则） 

#### Q6: Linux Cgroup & namespace

[LINUX CGROUP总结](https://www.cnblogs.com/menkeyi/p/10941843.html)

>  Linux CGroup全称Linux Control Group， 是Linux内核的一个功能，用来限制，控制与分离一个进程组群的资源（如CPU、内存、磁盘输入输出等）。
>
>  Linux CGroupCgroup 可让您为系统中所运行任务（进程）的用户定义组群分配资源 — 比如 CPU 时间、系统内存、网络带宽或者这些资源的组合。您可以监控您配置的 cgroup，拒绝 cgroup 访问某些资源，甚至在运行的系统中动态配置您的 cgroup。

> 主要功能：
>
> 1.限制资源使用，比如内存使用上限以及文件系统的缓存限制。
>
> 2.优先级控制，CPU利用和磁盘IO吞吐。
>
> 3.一些审计或一些统计，主要目的是为了计费。
>
> 4.挂起进程，恢复执行进程。

![](/Users/samstephen/Documents/Typora/image/区块链项目-Cgroup子系统.jpg)

> 内核使用 cgroup 结构体来表示一个 control group 对某一个或者某几个 cgroups 子系统的资源限制。cgroup 结构体可以组织成一颗树的形式，每一棵cgroup 结构体组成的树称之为一个 cgroups 层级结构。
>
> cgroups层级结构可以 attach 一个或者几个 cgroups 子系统，当前层级结构可以对其 attach 的 cgroups 子系统进行资源的限制。每一个 cgroups 子系统只能被 attach 到一个 cpu 层级结构中。

![](/Users/samstephen/Documents/Typora/image/区块链项目-Cgroup图解.jpg)

> Namespace：使用Namespace, 可以让每个进程组有独立的PID, IPC和网络空间.

#### Q7: 关于RPC请求

[知乎回答](https://www.zhihu.com/question/41609070/answer/191965937)

> 这个回答里恰巧讲了一些rpc通信协议的细节，但是强调一遍通信协议不是rpc最重要的部分，不要被这个回答带偏了。如果要了解rpc请更多的去了解服务治理(soa)的一些基本策略,推荐去看看dubbo的文档。
>
> 这个问题其实是有理解误区的，首先 http 和 rpc 并不是一个并行概念。
>
> rpc是远端过程调用，其调用协议通常包含传输协议和序列化协议。
>
> 传输协议包含: 如著名的 [gRPC]([grpc / grpc.io](https://link.zhihu.com/?target=http%3A//www.grpc.io/)) 使用的 http2 协议，也有如dubbo一类的自定义报文的tcp协议。
>
> 序列化协议包含: 如基于文本编码的 xml json，也有二进制编码的 protobuf hessian等。
>
> 因此我理解的你想问的问题应该是：**为什么要使用自定义 tcp 协议的 rpc 做后端进程通信？**
>
> 要解决这个问题就应该搞清楚 http 使用的 tcp 协议，和我们自定义的 tcp 协议在报文上的区别。
>
> 首先要否认一点 http 协议相较于自定义tcp报文协议，增加的开销在于连接的建立与断开。http协议是支持连接池复用的，也就是建立一定数量的连接不断开，并不会频繁的创建和销毁连接。二一要说的是http也可以使用protobuf这种二进制编码协议对内容进行编码，因此二者最大的区别还是在传输协议上。
>
> 通用定义的http1.1协议的tcp报文包含太多废信息，一个POST协议的格式大致如下
>
> ```text
> HTTP/1.0 200 OK 
> Content-Type: text/plain
> Content-Length: 137582
> Expires: Thu, 05 Dec 1997 16:00:00 GMT
> Last-Modified: Wed, 5 August 1996 15:55:28 GMT
> Server: Apache 0.84
> 
> <html>
> <body>Hello World</body>
> </html>
> ```
>
> 即使编码协议也就是body是使用二进制编码协议，报文元数据也就是header头的键值对却用了文本编码，非常占字节数。如上图所使用的报文中有效字节数仅仅占约 30%，也就是70%的时间用于传输元数据废编码。当然实际情况下报文内容可能会比这个长，但是报头所占的比例也是非常可观的。
>
> 那么假如我们使用自定义tcp协议的报文如下
>
> ![](/Users/samstephen/Documents/Typora/image/区块链项目-Q10.jpg)
>
> 报头占用的字节数也就只有16个byte，极大地精简了传输内容。
>
> 这也就是为什么后端进程间通常会采用自定义tcp协议的rpc来进行通信的原因。
>
> -- 分割线 2017.08.03 --
>
> 可能回答里面没有描述清楚，这个回答仅仅是用来纠正victory的回答的。所谓的效率优势是针对http1.1协议来讲的，http2.0协议已经优化编码效率问题，像grpc这种rpc库使用的就是http2.0协议。这么来说吧http容器的性能测试单位通常是kqps，自定义tpc协议则通常是以10kqps到100kqps为基准
>
> 简单来说成熟的rpc库相对http容器，更多的是封装了“服务发现”，"负载均衡"，“熔断降级”一类面向服务的高级特性。可以这么理解，rpc框架是面向服务的更高级的封装。如果把一个http servlet容器上封装一层服务发现和函数代理调用，那它就已经可以做一个rpc框架了。
>
> 所以为什么要用rpc调用？
>
> 因为良好的rpc调用是面向服务的封装，针对服务的可用性和效率等都做了优化。单纯使用http调用则缺少了这些特性。

#### Q8:无状态服务和有状态服务？

> 无状态服务：APACHE、负载均衡调度器
>
> 有状态服务：MySQL、DB

#### Q9: MVCC?

> MVCC（multi-version concurrency control，多版本并发控制）。

#### Q10：mmap（memory map）

[彻底理解mmap()](https://blog.csdn.net/Holy_666/article/details/86532671)

> memory map（内存映射），是一种内存映射文件的方法。

#### Q11：K8s中，Node结点的概念和pod、service是一个什么样的从属关系？

1. 首先，service就是将提供相同服务的pod进行分组，在需要相应服务的pod的时候，通过Cluster IP锁定到提供需要的服务的service组别中，再通过TargetPort获取一个pod来进行服务。也就是Service是pod的一个集合。
2. 

#### Q12：K8s中，Endpoint一会消失一会出现的问题

> 在使用K8s集群时遇到的问题：发现某个service的后端endpoint一会显示有后端，一会显示没有。显示没有后端，意味着后端的address被判定为notready。

> **经过排查确定原因：**
> kubelet在准备上报信息时，需要收集容器、镜像等的信息。虽然kubelet默认是10秒上报一次，但是实际的上报周期约为20~50秒。而kube-controller-manager判断node上报心跳超时的时间为40秒。所以会有一定概率超时。一旦超时，kube-controller会将该node上的所有pod的conditions中type是Ready的字典中的status置为False。

> **解决办法：**
> 较为简单的方案是在kube-controller上配置这个超时时间**node-monitor-grace-period**长一些。建议配置为**60 ~ 120s**。

